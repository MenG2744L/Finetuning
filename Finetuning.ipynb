{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MenG2744L/Finetuning/blob/master/Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UKgzOp-scbV"
      },
      "outputs": [],
      "source": [
        "#1安装微调库\n",
        "%%capture\n",
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "# 由于Colab有torch 2.2.1，会破坏软件包，要单独安装\n",
        "%pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "if major_version >= 8:\n",
        "    # 新GPU，如Ampere、Hopper GPU（RTX 30xx、RTX 40xx、A100、H100、L40）。\n",
        "    %pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "else:\n",
        "    # 较旧的GPU（V100、Tesla T4、RTX 20xx）\n",
        "    %pip install --no-deps trl peft accelerate bitsandbytes\n",
        "    %pip install xformers==0.0.25  #最新的0.0.26不兼容\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HriUxPMA5ezI"
      },
      "outputs": [],
      "source": [
        "%pip install xformers==0.0.25.post1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZecNgnL1wDi"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZEGjDAw1vUK"
      },
      "outputs": [],
      "source": [
        "#2加载模型\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcpr60L86bqE"
      },
      "outputs": [],
      "source": [
        "#3微调前测试\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "### Input:\n",
        "{}\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"请用中文回答\", # instruction\n",
        "        \"海绵宝宝的书法是不是叫做海绵体？\", # input\n",
        "        \"\", # output\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FcbYPqd8bYN"
      },
      "outputs": [],
      "source": [
        "#4准备微调数据集\n",
        "EOS_TOKEN = tokenizer.eos_token # 必须添加 EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # 必须添加EOS_TOKEN，否则无限生成\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"kigner/ruozhiba-llama3-tt\", split = \"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUMYKiPl_EFG"
      },
      "outputs": [],
      "source": [
        "#5设置训练参数\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, #  建议 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\", # 检查点，长上下文度\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # 可以让短序列的训练速度提高5倍。\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,  # 微调步数\n",
        "        learning_rate = 2e-4, # 学习率\n",
        "        fp16 = not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0zFaYZ-_MLt"
      },
      "outputs": [],
      "source": [
        "#6开始训练\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTcJODUjAy3M"
      },
      "outputs": [],
      "source": [
        "#7测试微调后的模型\n",
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"只用中文回答问题\", # instruction\n",
        "        \"火烧赤壁 曹操为何不拨打119求救？\", # input\n",
        "        \"\", # output\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLRoKZr356vd"
      },
      "outputs": [],
      "source": [
        "#8保存LoRA模型\n",
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "# model.push_to_hub(\"MenG2744/llama-3-8b-bnb-4bit\", token = \"---\") # 在线保存到hugging face，需要token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9合并模型并量化成4位gguf保存\n",
        "model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "model.save_pretrained_merged(\"outputs\", tokenizer, save_method = \"merged_16bit\",) #合并模型，保存为16位hf\n",
        "model.push_to_hub_gguf(\"MenG2744/llama-3-8b-bnb-4bit\", tokenizer, quantization_method = \"q4_k_m\", token = \"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\") #合并4位gguf，上传到hugging face(需要账号token)"
      ],
      "metadata": {
        "id": "AyLGCjaYD92m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#10挂载google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "nPAbbTGgJPw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11复制模型到google drive\n",
        "import shutil\n",
        "source_file = '/content/model-unsloth.Q4_K_M.gguf'\n",
        "destination_dir = '/content/drive/MyDrive/Llama3'\n",
        "destination_file = f'{destination_dir}/model-unsloth.Q4_K_M.gguf'\n",
        "shutil.copy(source_file, destination_file)"
      ],
      "metadata": {
        "id": "8Mpl85NUJR7l"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}